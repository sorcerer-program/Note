---
date: 2025-01-26
tags:
  - 机器学习
  - 算法
  - 集成
  - Boosting
---

## AdaBoost
AdaBoost全称为Adaptive Boosting，意为自适应提升算法，是一种用来<mark>分类</mark>的算法。
AdaBoost算法是通过<mark>改变样本的数据分布</mark>来拿到最优弱分类器的。AdaBoost会判断每次训练的样本是否分类正确，正确分类的样本权重降低，错误分类的样本权重增加。这样使得模型<mark>重视那些错误分类的样本</mark>，得到更高的分类准确率。

总结来说，AdaBoost算法有三个关键点：
1. AdaBoost将大量学习者组合在一起进行分类，弱学习者基本上是树桩。
2. 一些树在分类中比其他树桩更有发言权。
3. 每个树桩是考虑到之前树桩的错误而生成的。

AdaBoost的迭代过程是这样的：$$f_m(x) = f_{m-1}(x)+α_mG_m(x)$$要找的无非就两个：最优函数以及对应分类器的权重

完成AdaBoost需要三步：
<mark>第一件事：给每个样本一个权重，这个权重反应样本重要程度。</mark>
创建第一个树桩时，每个样本都是一样重要。权重 = 1/total
权重将发生变化，以指导如何创建下一个树桩。
<mark>第二件事：选取一个分类效果最好的特征进行分类（选取最优函数）</mark>
计算基尼系数，选取基尼系数最低为最好的特征。
<mark>第三件事：确定这个树桩在最终分类中有多少发言权。</mark>
基于样本的分类程度，x = error/total
我们用这个式子来确定这个树桩在最终分类中有多少发言权$$\frac{1}{2}log(\frac{1-x}{x})$$![[Pasted image 20250125233212.png]]
通过图形可以看出来，x越靠近1，对应的数值就越小，x越靠近0，对应的数值就越大。
如果有一个误差（x）为0或者1，那这个模型就坏掉了。

我们需要注意的是，第一步当中的给每个样本一个权重，后续是需要变化每个权重的。
接下来，我们要修改权重，以便于下一个树桩注意到当前树桩产生的错误。
最开始，所有样本权重都是相同的，这意味着我们没有强调正确分类任何特定样本的重要性。

我们首先增加错误分类样本的权重。
我们使用这个式子来进行计算：$$New\ Sample\ Weight=sample\ weight×exp(-amount\ of\ say)$$
![[Pasted image 20250125235526.png]]
$$exp(-amount\ of\ say)$$
这个式子表示，发言资格越大（分类效果越好）权重增量就越小；而发言资格越小（分类效果越差）权重增量就越大。通过计算 New Sample Weight 来减少所有正确分类样本的样本权重，增加所有错误分类样本的样本权重。

这样之后，样本权重就分别增大或减小了，但是值得注意的是，毕竟是权重，总和应当为1，我们对新的权重进行一个归一化，Norm Weight = new weight / total new weight .

对于第二步，选取最优函数，基尼系数不是唯一的方法。
选取最优树桩可以使用两种方法：
1. 计算加权基尼系数
2. 基于样本权重来生成新的训练集
我们对求加权基尼系数不在赘述，我们说说生成新的训练集这个方法，也就是加权抽样。
![[Pasted image 20250126004905.png]]
通过随机数r，选取放入新训练集当中的样本。样本权重越大越容易被抽中。

我们对比这两种方式，基尼系数更为直观，会更加精确，但是计算的复杂度高，特别是样本量较大和特征多的时候开销明显很大。创建新训练集精确度没有基尼系数高，同样计算复杂度也没有那么大。

总结下来：如果数据量不大且需要精确选择，加权基尼系数会好一些，其他时候加权抽样方法会更好（大数据计算效率更好；噪声较多或难分类样本多，加权抽样鲁棒性更好；多轮训练模型稳定性更好）

## XGBoost
XGBoost旨在用于大型复杂数据集

