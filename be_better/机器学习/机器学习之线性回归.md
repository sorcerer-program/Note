---
date: 2024-10-28
tags:
  - 机器学习
  - 算法
  - 线性回归
  - 最小二乘法
  - 梯度下降
---
# 线性回归
## 什么是线性回归
### 什么是回归
> 回归问题是函数拟合问题，就是给定一些点的集合，用一个曲线或者方程去拟合，使得所有点都大致符合
### 什么是线性回归
> 拟合的是一条直线，那就是线性回归
## 实现基础的线性回归
### 实现回归的本质
拿到一个*好方程*，输入已知量(自变量)，就能通过这个方程预测未知量(我们想要的数据)
### 拟合的方程
我们假设能够拟合的方程是:
$$
h_\theta(x) = \theta_0 + \theta_1 x_1 + \theta_2 x_2 + \ldots + \theta_n x_n
$$
为了一般化这个方程，我们引入常量$x_0$=1
$$ h_\theta(x_{n\times1)} = \sum_{i=0}^{n} \theta_i \cdot x_i = 
\theta_{n\times1}^Tx_{1\times n}$$
要实现回归，就找到一群系数$\theta$，也就是一个$\theta$向量，也称作特征分量。
### 拟合的好坏评价
#### 损失函数
这个函数用来衡量我们预测值和真实值之间的差距
$$ J(\theta_{n\times1}) = \frac{1}{2}\sum_{i=1}^m(h_\theta(x_{n_\times1})^{(i)} - y^{(i)})^2$$
>(预测值-真实值)的平方，然后对每一组训练数据进行累加
>需要一提的是，$\frac{1}{2}$不是必要的，只是为了简化推导（最小二乘法的思想）

损失函数的值小，就表明我们的误差小，那么我们的问题就变成了求最小值。
求$${minJ\theta}$$
### 拟合的算法
#### 最小二乘法
显然损失函数是一个关于$\theta$的二次函数，二次函数的图像不外乎像这样：
![[Pasted image 20241028030501.png]]
导数为零的时候自然就是损失最小的时候
求损失函数关于$\theta$的导数：
$$导数 = 2{x^T_{n\times1}}(y_{n\times1} - x_{n\times1}\theta_{n\times1})$$
令导数为零，得到：
$$\theta_{n\times1} = (x^T_{n\times1}x_{n\times1})^{-1}x^T_{n\times1}y_{n\times1}$$
也就是说代入这个公式，就能拿到合适的$\theta$向量，也就达成了目标。
代码实现也不是很困难
```python
import numpy as np
def standRegres(xArr, yArr):  
    """  
    函数说明：计算回归系数theta  
    :param xArr: x数据集  
    :param yArr: y数据集  
    :return: 无  
    """    xMat = np.asmatrix(xArr)  
    yMat = np.asmatrix(yArr).T  
    xTx = xMat.T * xMat  
    if np.linalg.det(xTx) == 0.0:  
        print('矩阵为奇异矩阵，不能求逆')  
        return  
    theta = xTx.I * (xMat.T * yMat)  
    return theta
```

*但是我们应该发现，这个公式中含有逆矩阵，然而现实中往往数据不可逆，因此最小二乘法不能适用于所有模型，而且，我们希望模型是不断从数据样本中学到有用的东西，而不是一步求解。*
我们应该找一种更普遍的方法，目光看向梯度下降。
#### 梯度下降法
##### 理解思路
直观说梯度下降法的话，就是：
> 我站在高山上，我想用最短的时间下山，但是每次只能走一步。那么我需要做的就是环顾一周，找到一个最陡峭的方向，然后移动到那个点上；到新位置之后，重复刚才的动作。每次都选择最陡峭的方向走，那么很快就能下山。

就像这样：
![[Pasted image 20241026163100.png]]

条条道路通罗马，但是从图像来看，不同起点到的最低点不一定一样
那是不是需要从所有地方出发，拿到所有情况呢？

答案是否定的
因为损失函数的图长这样：
![[Pasted image 20241026163114.png]]
不用担心，它就是一个碗而已。

既然行得通，我们就放心考虑思路
最小梯度法的整体思路就是：
	1. 对$\theta_{n\times1}$进行赋值，这个值随机，通常复值一个全零的向量
	2. 不停迭代，每次迭代都改变$\theta_{n\times1}$，使得$J(\theta_{n\times1})$按照梯度下降的方向进行减少

##### 具体落实
确定了下山要一步一步走，具体怎么落实呢？
很简单：找一个方向，走一步

* __找一个方向__
> 下山哪个方向才是最陡峭的呢？
	首先想到的是斜率，在我数学知识不多的印象中：斜率越大越陡峭
	通过斜率又不难想到导数，所以这定是与导数相关的。
	简而言之就是*这一步*由*上一步的基础*和*某个导数*确定$$这一步 = 上一步 ？ 某导数$$
	推不下去了，就拿出结论吧：$$\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_{n\times1})$$
	
* __走一步__
> 一步走多远？
> 上面式子中的$\alpha$称为学习率(learning rate)，直白的说就是每一步的步长。
> 	1.$\alpha$太大可能错过最小值，最后不收敛
> 		想象一下一个人一步可以跨的很大，大到可以从这个山头跨到那个山头，那么这个人就永远不能下山，一直在两个山头反复横跳
> 	2.$\alpha$太小又会迭代很多次，消耗资源
> 		想象一下小碎步下山
> 拿张图就明白了
> ![[Pasted image 20241027162552(1).png]]

下山的关键就是那个迭代公式！
$$\theta_j = \theta_j - \alpha\frac{\partial}{\partial\theta_j}J(\theta_{n\times1})$$
对损失函数 $J(θ)$ 关于参数 $θ$ 的偏导数（梯度）是：
$$∇J(θ) = \frac{\partial J(\theta)}{\partial\theta} = \frac{2}{m} X^T(X\theta - y)$$

数学转换，得到：
$$θ = θ - \alpha(\frac{2}{m} X^T(X\theta - y))$$
```python
import numpy as np

# 生成一些示例数据
np.random.seed(0)
X = 2 * np.random.rand(100, 1)
y = 4 + 3 * X + np.random.randn(100, 1)

# 增加偏置项
X_b = np.c_[np.ones((X.shape[0], 1)), X]  # 在X的前面加一列1

# 超参数
alpha = 0.1  # 学习率
iterations = 1000  # 迭代次数

# 初始化权重
theta = np.random.randn(2, 1)  # 随机初始化权重

# 梯度下降算法
for i in range(iterations):
    predictions = X_b.dot(theta)  # 计算预测值
    errors = predictions - y  # 计算误差
    gradients = (2 / len(X_b)) * X_b.T.dot(errors)  # 计算梯度
    theta -= alpha * gradients  # 更新权重

# 打印最终的权重
print("最终的权重:", theta)

```

学习率通过不断尝试，得到最佳

还值得一提的是：
##### 提高效率

为什么缩放？
特征值的范围是有差距的，有的特征影响大，有的影响小
就像你要竞选主席，你是中国公民这一特征的影响就远远小于你政治能力地位这一特征
![[Pasted image 20241027150630.png]]

为了让机器在读取数据的时候感觉更“舒服”，训练起来效率更高，还需要进行特征的缩放
![[Pasted image 20241027150856.png]]

特征缩放的常用方法
* 最大值缩放 ![[Pasted image 20241028060744.png]]
* 均值归一化![[Pasted image 20241027151409.png]]
* Z-score 标准化![[Pasted image 20241027151659.png]]