---
date: 2025-01-19
tags:
  - 机器学习
  - 算法
  - 集成
  - 同质
---
# 同质集成
## 集成方法简介
我们进行学习的时候，使用多个学习器往往会取得更好的效果。集成方法的思路就是集成多个学习器。
不同种学习器集成起来，称作异质集成。
相同种学习器集成起来，称作同质集成。
此篇文章，我们主要进行同质集成方法的讨论。
同质集成当中，我们有Boosting（提升集成）、Bagging（自助集成）、Stacking（堆叠集成）、Voting（投票集成）
## Boosting
Boosting是一种通过将多个弱学习器（单独效果较差的学习器）组合起来，产生一个强学习器的的集成学习方法。Boosting方法的核心是根据<mark>每一轮的错误</mark>来调整训练数据的权重，使得后续模型能够更好地学习之前模型错误的样本。

我很喜欢[机器学习经典算法之AdaBoosting](https://www.cnblogs.com/jpcflyer/p/11268859.html)这篇博客中的一句话：臭皮匠好训练，诸葛亮不好求。我们可以通过训多个臭皮匠来达到一个诸葛亮的效果。
![[Pasted image 20250119183529.png]]
Boosting的内核在于：弱可学习的算法组合出来一个强可学习的算法。
$$f(x)=\sum_{i=1}^nα_iG_i(x)$$
1. 弱学习器：如何拿到每一个Gi(x)
2. 组合：如何确定每个弱学习器对应的权重αi

### PAC框架
Boosting是借助弱可学习（比随便猜就好）构建成强可学习，我们将强可学习成为PAC可学习（PAC-learnable）。PAC全称probably approximately correct （渐进概率上的一个正确率）。错误率足够小，渐进变成一个必然事件。
### AdaBoost
AdaBoost全称为Adaptive Boosting，意为<mark>自适应</mark>提升算法。自适应体现在：在每一轮迭代中根据前一轮的错误调整样本权重。
$$f_m(x) = f_{m-1}(x)+α_mG_m(x)$$这个式子能够表达出AdaBoost的迭代过程。
第一步，如何确定每个弱分类器？
AdaBoost算法是通过<mark>改变样本的数据分布</mark>来实现的。具体来说就是AdaBoost会判断每次训练的样本是否分类正确，对于正确分类的样本，降低权重，对于错误分类的样本，增加权重。再基于上一次得到的分类准确率，来确定这次样本中每个样本的权重。然后将修改过权重的新数据集传递给下一层的弱学习器进行训练。这样来的话，错误的样本会越来越受重视。
这样看来，AdaBoost最具特色的点在哪呢？显然是样本数据的权重（重视误分类点）。
每个样本有着对应的权重，我们表示为$$D_{k+1}=(W_{k+1,1}，W_{k+1,2}，……，W_{k+1,n})$$
第K+1轮中的样本权重，是根据该样本在第K轮的权重以及第K个分类器的准确率来确定的，公式是这样的：
$$W_{k+1,i}=\frac{W_{k,i}}{Z_k}exp({-α_{k}y_{i}G_k(x_i)})$$
怎样来表示损失函数呢？
指数损失$$L(y,f(x))=exp(-yf(x))$$分类结果用1和-1来表示，预测与实际一致，则指数为负，损失函数小，若预测与实际不一致，则指数为正，损失函数大。



## AdaBoost例题感受
![[Pasted image 20241223204742.png]]
![[Pasted image 20241223211024.png]]
![[Pasted image 20241223212153.png]]
## AdaBoost算法简介
![[Pasted image 20241223213905.png]]
## 算法中的权重更新
![[Pasted image 20241223214425.png]]
费马定理![[Pasted image 20241223215303.png]]
![[Pasted image 20241223215123.png]]![[Pasted image 20241223232736.png]]
## 训练误差的上界
![[Pasted image 20241224081026.png]]![[Pasted image 20241224081602.png]]![[Pasted image 20241224082440.png]]![[Pasted image 20241224102230.png]]
![[Pasted image 20241224102957.png]]![[Pasted image 20241224103356.png]]
## 训练误差的上界的上界
![[Pasted image 20241224104311.png]]![[Pasted image 20241224104339.png]]![[Pasted image 20241224104647.png]]
![[Pasted image 20241224105220.png]]
![[Pasted image 20241224110103.png]]
![[Pasted image 20241224110440.png]]
![[Pasted image 20241224110726.png]]
![[Pasted image 20241224111133.png]]将连乘化为求和（为什么要证明）
![[Pasted image 20241224111717.png]]
为什么是-2x
![[Pasted image 20241224111941.png]]
推论
![[Pasted image 20241224112542.png]]
## 回归中的前向思想
Adaboost前向分布算法  
逐步回归
![[Pasted image 20241224193018.png]]
![[Pasted image 20241224193644.png]]
![[Pasted image 20241224194119.png]]
## 可加模型的原理
 additive model（可加模型，加法模型）
![[Pasted image 20241224201328.png]]
 Adaboost的前向分步算法![[Pasted image 20241224202629.png]]
![[Pasted image 20241224204443.png]]
 ## 提升树![[Pasted image 20241224210017.png]]
![[Pasted image 20241224211446.png]]
![[Pasted image 20241224212003.png]]
![[Pasted image 20241224212212.png]]
![[Pasted image 20241224212441.png]]
![[Pasted image 20241224213519.png]]

 
  