## 决策树算法概述
类比猜人物游戏
## 熵的作用
目标：通过一种衡量标准，来计算通过不同特征进行分之选择后的分类情况，找出来最好的那个当根节点，从此类推。

衡量标准：熵。
随机变量不确定性的度量（混乱程度）
经过一次决策后，熵值的一个结果。

## 信息增益原理
![[b715cba7dfb271c8717472be3dac8e0.jpg]]
![[c1bc3164455ba4ea76119719ddcfaf7.jpg]]
<mark>信息增益</mark>：表示特征X使得类Y的不确定性减少的程度。（分类后的专一性，希望分类后的结果是同类在一起）

特征怎么选，是通过信息增益来确定的。

## 决策树构造实例
![[01d76490208a728dd642ffff9db1c15.jpg]]
![[199614a3f084d31c081e2b45b7fc5a4.jpg]]
## 信息增益率与gini系数
![[e9829d304822965655803bd3f8a27a2.jpg]]
(ID3)信息增益：如果有id的时候，id就是最优的特征。
(C4.5)信息增益率：考虑自身熵，解决ID3问题。![[17c2faa9edb62820723c47d104e0360.jpg]]
 
(CART)使用GINI系数来当做衡量标准：和熵的衡量标准类似，计算方式不相同。

连续值怎么半？上面的情况都针对的离散值。
首先进行排序。
若进行二分，则有很多种情况。
选取（连续值的）哪个分界点？
<mark>贪婪算法</mark>
哪个点的效果最好我们就在哪个点去做这样一件事
连续值离散化。
## 预剪枝方法
为什么剪枝：决策树过拟合风险很大，理论上可以完全分得开数据(树足够大，每个叶子节点就只有一个数据)
什么叫做过拟合呢？就是这个数据集在你训练的时候，它做得很好，但是实际测试的时候他就不太好了。

预剪枝：边建立决策树边进行剪枝的操作(更实用)
我们不希望决策树过于复杂，也不希望它过于简单。你可以设置一些参数，这些参数来去控制模型的一个增长。
 到底哪种参数合适呢？通过实验来不断进行观察。交叉验证，选一组试试，看哪组参数在交叉验证上结果得到好。遇见实际问题的时候，不要总相信经验值，自己试一试。经验值仅供参考。

## 后剪枝方法
后剪枝：当建立完决策树后来进行剪枝操作。
![[dc801bc724daa7065cf0e20caec7bff.jpg]]
α是我们自己设定的一个值
α越大，就是希望数据模型越不过拟合，但是得到的结果可能不是那么好。
α越小，就是希望结果好为主，过不过拟合可能没有那么重要。
剪完之后损失反而小就把它剪掉，如果变大就不要剪。

## 回归问题解决
到了一个叶子节点，这个叶子节点到底是属于什么属性？是由什么决定的？是由于当前叶子节点所有样本的一个众数决定的。

回归怎么办？不能判断熵值，因为回归当中它没有一个具体类别。我们 可以用<mark>方差</mark>，方差能够表达数据之间的一个离散程度。差异程度越大，方差越大。我们将方差当做衡量标准。
回归当中预测的结果是数据之间的平均数。

## 随机森林

决策树就像个人的意见，而随机森林是人群的共识。

![[Pasted image 20241216184923.png]]
Bagging：取平均     并联
Boosting：提升        串联
Stacking：堆叠
### Bagging
![[Pasted image 20241216185410.png]]

森林：多棵树并行做分类，然后取众数

用相同的数据去训练，得到的树肯定是相同的，所以<mark>数据一定要随机</mark>，这样森林才会有意义。

数据随机采样，特征随机选择。两种随机，使得建立起来的树模型互相不同。

![[Pasted image 20241216190244.png]] 