---
date: 2024-10-28
tags:
  - 机器学习
  - 算法
  - 逻辑回归
---
# 逻辑回归
## 什么是逻辑回归？
> 逻辑回归是根据给定的数据集估算时间发生的概率，是一种分类

> 明明是分类为什么称作回归呢？
> 因为它的底层方法就是线性回归，逻辑回归是基于回归的伪回归算法。

## 实现基础的逻辑回归
### 理解逻辑回归
#### 逻辑函数的表达
回想线性方程
$$Z = W^TX + b$$
对于直线，我们知道z的输出范围没有任何的限制，可以从负无穷到正无穷。但是，我们需要的是一个分类器，想要拿到两种结果，0或者1，所以我们需要输出范围为（0,1）的式子。
在线性方程的基础上达到规范输出范围这一目的，我们使用了Sigmoid函数$$σ(z) = \frac{1}{1+e^{-z}}$$![[Pasted image 20241028195414.png]]
图像很像s，就跟sigmoid沾上边了，所以拿到一个逻辑函数的名称。

我们在z的基础上进行的操作来达到控制范围的效果，我们就将z代入，就像下面这样。
![[Pasted image 20241028182720.png]]
![[Pasted image 20241028190843.png]]
我们要明确最初的目标，是分类，只有两种结果，要么0要么1，那哪些判断为0哪些判断为1呢？
我们引入决策边界。边界内为一种情况，边界外为一种情况。
不难想到会有线性边界以及非线性边界
情况类似下图
![[Pasted image 20241028200239.png]]
![[Pasted image 20241028183305.png]]

> 所以，逻辑回归的整体思路是，先用逻辑函数把线性回归的结果 (-∞,∞)映射到(0,1)，再通过决策边界建立与分类的概率联系

在二分类模型中，事件发生于不发生的概率之比$$\frac{p}{1-p}$$称作事件的几率。
几率的范围也是(0,1)，那我们就令
$$z = log(\frac{p}{1-p})$$
也就是说，线性回归的结果等于对数几率。

#### 损失函数
自然，向线性回归一样，逻辑回归也需要我们判断拟合好与不好。

线性回归分析梯度下降时，我们就提出了，条条大路不一定都能通罗马，恰好线性回归的损失函数是一个碗罢了，只有一个最小。
那逻辑回归还会这么幸运吗？
并不是，逻辑函数的平方损失函数布满了局部最小值。
![[Pasted image 20241028183938.png]]
![[Pasted image 20241028191058.png]]
这样一来，梯度下降难以实行啊。为了解决这个问题，我们得让它也像碗一样听话。

痛苦的根源是什么？有个e啊！还在分母上，怎么导也动不了啊！联想一下高考导数题，不难想到取对数。试一试，取对数情况很不错。

y要么为0要么为1，看一看对数的图像
![[Pasted image 20241028202156.png]]
如果是一根线，那么不管怎么个情况，0和1对应取到的值都是天壤之别。

一根不能满足，两根就可以。一正一负，很合适。看看下面三张图：
![[Pasted image 20241028185200.png]]
![[Pasted image 20241028184731.png]]
![[Pasted image 20241028185001.png]]

取对数，图形就变得很乖了，看不出坑坑洼洼。
![[Pasted image 20241028191229.png]]
这下就适合梯度下降了。

化简一下：
![[Pasted image 20241028192104.png]]

接下来就是线性回归一样的梯度下降处理了
![[Pasted image 20241028192602.png]]

不难看出，求的偏导跟线性回归好像一样。
但是区别就在于f的表达式。

剩下的处理跟线性回归也就大差不差了。详情可见[[机器学习之线性回归]]

我想要使用鼠标为什么不行？远程