---
date: 2024-11-03
tags:
  - 机器学习
  - 聚类
  - K-Means
---
# 聚类
## 什么是聚类
> 一个班级很多人，随着时间的推移，会形成几个小团体，这个找朋友的过程就叫做聚类。人以类聚，物以群分，数据亦如此。

> 聚类是最常见的无监督学习算法，把一个数据集按照某个特定标准(比如距离)分割成不同类或簇，使得同一个簇内的数据相似性尽可能大，不同簇差异尽可能大
![[Pasted image 20241103201908.png]]
## 有哪些聚类
找朋友肯定要有一定的标准吧，比如我是柯哀党，你是新兰党，肯定是水火不容不能分为一类的。根据不同分割标准，我们有很多种的聚类算法，下图陈列了一些。其中k-means是一种非常经典的聚类算法，古老但实用。本文主要针对k-means进行讨论。
![[Pasted image 20241103203619.png]]
## 怎样实现聚类
### K-Means
#### 理解
<mark>k-means是基于划分的聚类</mark>
##### 质心
一个团体总有一个核心，比如中国共产党代表全国人民最根本的利益，所有人民都围绕着中国共产党。数据也有一个聚类中心，一类数据(一个簇)会围绕着一个聚类中心。我们将这个聚类中心称作**质心**，我们用μ来表示。
$$μ_i = \frac{1}{|C_i|}\sum_{x \in C_i} x$$
(第i个簇的质心μ是该簇内所有数据点的平均值，也就是中心点)

##### 损失函数
要评价类分得好不好，我们自然要引进损失函数，也称失真函数。
$$J(c^{(1)},...c^{(m)}, \mu_1,...\mu_K) = \frac{1}{m}||x^{(i)} - \mu_c^{(i)}||^2$$
失真函数是表现的簇内点到簇中心的距离平方。距离越远，说明这个团队不够紧密，也就说明这个类聚得不好。
![[Pasted image 20241104172257.png]]
我们要使得距离近有什么办法呢？
* 第一点，每个样本点划分到最近的中心点队列。（群众要紧紧拥护党的领导）
>众多的簇内点应该选取合适的簇中心。如下图，x距离红色质心的距离会远小于蓝色质心的距离。
![[Pasted image 20241103125055.png]]
* 第二点，质心应该应该到合适的地方。（党应该深入人民群众）
> 质心要到中心去，这样总体的距离之和才会小。这是一个数学问题，本质上是高中均值不等式当中的平方均值不等式。
> $$\frac{a + b}{2} \geq \sqrt{\frac{a^2 + b^2}{2}}  （a=b时等号成立）$$
> 如图：![[Pasted image 20241103125128.png]]
> 显然有a+b=10，那么有
> ![[Pasted image 20241104164538.png]]
> 也就是说取均值时，距离平方和最小。所以我们应该算取每个类别中样本点的均值，将均值作为新的中心点。
> ![[Pasted image 20241104172908.png]]

要实现下来，总共就两步：将样本点分配给最近的质心，根据分配重新计算质心。
K-means关键就是这两步，一次一次迭代，最终就能聚类。

还需要注意的是：
1. 初始化质心
> 最开始是没有质心的，也就没有办法进行分配，所以我们应该初始化质心。怎么初始化，可以取随机的样本点。

> K-Means的质心特别重要，十分敏感。如果最开始的质心没有被分配点，那么求平均值是没有意义的。最常见的做法是消除没有点集群，得到k-1个簇。
> ![[Pasted image 20241104173001.png]]
> 我们可以使用K-means++，详情可以参考[[K-Means++]]

2. 选择合适的K值（也就是合适的质心数量）
> K值选择不当，会造成过拟合或欠拟合。我们常常选择“肘部法”。
> ![[Pasted image 20241103132931.png]]
> 有一个值，过了这个值，损失函数下降的幅度比较小，这个就是我们需要的K。
> 值得一提的是，选择K值并不完全根据损失来定，我们要结合实际情况。
##### 代码实现
代码实现迭代的k-means的关键两步：
1. 将样本点分配给最近的质心：
```python
def find_closest_centroids(X, centroids):  
    """  
    计算每个实例的质心隶属度  
    :param X: 形状为(m,n)的numpy数组，其中m是样本数量，n是特征数量。X中包含了m个数据点，每个数据点有n个特征  
    :param centroids: 形状为(k,n)的numpy数组，k是质心的数量，每个质心也有n个特征  
    :return: 一个形为(m,)的数组，表示每个数据点最近的质心的索引  
    """  
    # 获取centroids的第一维度，就是获取质心的数量K 
    K = centroids.shape[0]   
    # 创建一个全零的数组idx，长度为m，用于存储每个数据点最近的质心的索引  
    idx = np.zeros(X.shape[0], dtype=int)
    for i in range(X.shape[0]): # 每个实例  
        distance = []  
        for j in range(K): # 每个质心  
	        # 欧几里得距离来进行计算距离  
            norm_ij = np.linalg.norm(X[i] - centroids[j]) 
            distance.append(norm_ij)  
        # 找到最近质心的索引  
        idx[i] = np.argmin(distance)    
    return idx
```
2. 使用分配给每个质心的点重新计算平均值: 
```python
def compute_centroids(X, idx, K):  
    """  
    用于计算新的质心，分配给每个质心的数据点的平均值  
    :param X::param idx::param K::return:   
    """  
    m, n = X.shape  
    # 创建一个形状为(K,n)的numpy数组，用来存储计算得到的新质心  
    centroids = np.zeros((K, n))    
    # 计算新质心  
    for k in range(K):  
	    # 找到所有分配给当前质心k的数据点  
        points_assigned_to_k = X[idx == k]  
        if points_assigned_to_k.size > 0:   # 确保有点被分配  
            centroids[k] = np.mean(points_assigned_to_k, axis=0)  
    return centroids
```

### 谱聚类
#### 理解
<mark>谱聚类是基于图的聚类</mark>

聚类可以理解为：让距离更近的样本聚集在一起，距离远的样本分到不同的簇中。
我们可以用权重来表示距离大小(相似性)。那么聚类的过程就可以说成把数据集切开，切成需要的份数，最终的结果就是每个子集里面的样本呢权重尽可能大，子集之间被切断的权重尽可能小。那么聚类的关键问题就是：<mark>我们应该怎么去切这个数据集？</mark>而谱聚类的关注点就是解决这个问题。
##### 谱聚类的整体步骤
1. 基于已有的数据集进行构图
2. 基于构好的图进行切图
##### 构图
我们要进行构图，要构造什么样的图呢？
介绍一些索然无味的概念。

>对于一个图G，我们通常描述为G(V,E)  其中的V是点的集合(也就是我们的数据集)，E是边的集合。我们定义两个点之间的权重为w，由于是无向图w_ij = w_ji 。 有连接的两个点w>0，没有连接的两个点，w=0。



> 对于任意一个点，我们给它定义一个度（d_i），一个点的度就是这个点和它相连的所有点的权重之和。$$ d_i = \sum_{j=1}^{n} w_{ij} $$
> 我们再在度这个概念之上，定义一个度矩阵，度矩阵是一个n阶主对角方阵，n就是图中标的节点数。
> $$D = \begin{pmatrix}
d_1 & 0 & \cdots & 0 \\
0 & d_2 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & d_n \\
\end{pmatrix}$$
> 有两个令人好奇的点，一是这个矩阵为什么要定义成方阵呢？二是为什么只有主对角线上有数据呢？
> 为什么是方阵？这是一个很有趣的问题，但是答案很无趣，因为定义。只是为了将矩阵的维度与节点的数量进行匹配。
> 为什么只有主对角线上有数据？因为度只跟该节点本身有关，不涉及节点之间的具体连接关系。非对角线上的0表示节点之间没有直接的联系。

>  一个图我们已经有点集合了，那关键的就是边，也就是权重，我们把所有样本点的权重组成的矩阵，称之为邻接矩阵W，邻接矩阵也是一个n阶方阵。
>  $$
W = \begin{bmatrix}
a_{11} & a_{12} & \cdots & a_{1n} \\
a_{21} & a_{22} & \cdots & a_{2n} \\
\vdots & \vdots & \ddots & \vdots \\
a_{n1} & a_{n2} & \cdots & a_{nn}
\end{bmatrix}
$$
>  构建邻接矩阵方法主要有三种：  ϵ-邻近法，K邻近法、全连接法

我们讨论一些这三种方法：
1. ϵ-邻近法
	s_ij是点i和点j之间的欧氏距离，我们提出一个距离阈值ϵ。我们来定义邻接矩阵：$$
w_{ij} = 
\begin{cases} 
0, & \text{if } s_{ij} > \epsilon \\ 
\epsilon, & \text{if } s_{ij} \leq \epsilon 
\end{cases}
$$
	这样邻接矩阵只有两种数据，表达的也只是有边还是没边。
	比如： $$
W = \begin{bmatrix}
0 & 1 & \cdots & 1 \\
1 & 0 & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 1 & \cdots & 0
\end{bmatrix}
$$
	其中a_12表示节点1和节点2有边(有联系)，若为0则表示没边。
	这个方法的话表示距离远近就很不理想。
2. K邻近法
	利用遍历所有的样本点，取每个样本最近的k个点作为近邻，只有和样本距离最近的k个点之间有权重w>0。
	![[Pasted image 20241104194640.png]]
3. 全连接法
	$$
w_{ij} = s_{ij} = \exp\left(-\frac{||x_i - x_j||^2}{2\sigma^2}\right)
$$
全连接法是建立邻接矩阵最普遍的方法。

构图方面，我们再提及一个<mark>拉普拉斯矩阵</mark>(L)
其实非常简单：L = D - W

###### 切图
我们的目标是将图G(V,E)切成相互没有连接的多个子图。

针对任意两个子图点的集合A,B⊂V，A∩B=∅，我们提出切图权重：$$
W(A, B) = \sum_{i \in A, j \in B} w_{ij}
$$
明显的，切图权重表达的是1图中的所有点，和2图中的所有点，两两权重之和。
如果这个切图权重大，就说明两个图中的点联系紧密，说明切得不好。

我们针对所有的子图（假设一共有m个）它们图中的点集是A_1, A_2……A_m，我们定义cut：
$$
cut(A_1, A_2, \ldots, A_m) = \frac{1}{2m} \sum_{i=1}^{m} W(A_i, \overline{A_i})
$$
这个式子表现了所有子图图之间的权重，最小化cut，那就使得子图联系最小，达成了切图。切成了一个又一个联系小甚至没有联系的子图。

![[Pasted image 20241104201735.png]]

如何去使得这个cut最小？我们提出两种切图方法：
RatioCut切图和Ncut切图。


