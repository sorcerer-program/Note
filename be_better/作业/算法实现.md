# 算法实现（ SVM 和 决策树）
## SVM（分类）
### 概述
SVC 是 SVM 中的一种，用于寻找最优超平面（Hyperplane），以最大化类别之间的间隔（Margin），从而实现分类。

通过数学推导，目标函数为：$$\min_{\alpha_i} \left( \frac{1}{2} \sum_{i=1}^{n} \sum_{j=1}^{n} \alpha_i \alpha_j y_i y_j \mathbf{x}_i^T \mathbf{x}_j -\sum_{i=1}^{n} \alpha_i  \right) ,\quad \text{s.t.  } \sum_{i=1}^{n}α_iy_i=0，0\leq α_i \leq C$$其中的 α 是引入的拉格朗日乘子，数量与样本数量一致。当数据量大的时候，用梯度下降来整体优化计算开销大。为了解决这个问题，我们使用启发式算法 SMO ，每次只优化一对 alpha ，通过分解变量众多的问题，将其转化为一个较小规模的二次规划问题，进而逐步解决。

最后的决策函数表示为：$$f(x) = w^T x + b = \sum_{i=1}^{n}α_iy_ix_ix+b$$
### 逻辑推理

	对数据集中的每个 alpha ：
		随机选择另一个 alpha 
		同时优化这两个 alpha
	返回所有优化好的 alpha

上面的逻辑遍历了一遍 alpha ，理当优化了所有 alpha 。但 SMO 不是一个整体优化过程，更新一对 alpha 会对前面更新过的 alpha 产生影响（原本是最优的alpha对因为下一对的更新有可能变得不是最优）。如此，遍历一遍是远远不够的。我们在遍历的基础上加一个外循环。我们记录连续停更次数。<mark>当 alpha 连续停更次数达到一定值，说明 alpha 已经最优</mark>。同时避免死循环，我们设置一个最大迭代次数 。

	当迭代次数<最大迭代次数 and 连续停更次数<设定值：
		alpha 更新次数 = 0
	
		内循环：
			优化 alpha
			alpha 更新次数 + 1
	
		如果 alpha 更新次数 == 0：
			iter + 1
		否则：
			连续停更次数 + 1

### 代码实现
#### 信息存储
首先我们需要一个数据结构来放置信息，我采用了一个 Information 类：
```python
class Information:  
    def __init__(self, x, y, tol, c):  
        self.x = x  
        self.y = y  
        self.m = x.shape[0]  
        self.alpha = np.zeros(self.m)  
        self.K = np.zeros((self.m, self.m))  
        self.e_cache = np.zeros((self.m, 2))  
        self.tol = tol  
        self.c = c  
        self.b = 0
```
#### 内循环
内循环起到的作用是遍历优化 alpha 数组
```python
for i in range(infor.m):  
    alpha_changed += optimize_alpha(infor, i)
```
其中调用的`optimize_alpha(infor, i)`用于优化一对 alpha
```python
def optimize_alpha(infor, i):  
    """优化一对 alpha """    
    e_i = calculate_and_update_e(infor, i)  
    if (infor.y[i] * e_i < -infor.tol and infor.alpha[i] < infor.c) or \  
        (infor.y[i] * e_i > infor.tol and infor.alpha[i] >0):  
        j, e_j = select_j(infor, i)  
        alpha_i_old = infor.alpha[i].copy()  
        alpha_j_old = infor.alpha[j].copy()  
        eta = infor.K[j, j] + infor.K[i, i] - 2 * infor.K[i, j]  
        alpha_i = infor.alpha[i] + infor.y[i] * (e_j - e_i) / (eta + 1e-5)  
        calculate_and_update_e(infor, i)  
        zeta = infor.alpha[i] * infor.y[i] + infor.alpha[j] * infor.y[j]  
        if infor.y[i] == infor.y[j]:  
                lower = max(0, zeta / infor.y[i] - infor.c)  
                upper = min(infor.c, zeta / infor.y[i])  
        else:  
            lower = max(0, zeta / infor.y[i])  
            upper = min(infor.c, zeta / infor.y[i] + infor.c)  
        alpha_i = np.clip(alpha_i, lower, upper)  
        alpha_j = (zeta - infor.y[i] * alpha_i) / infor.y[j]  
        calculate_and_update_e(infor, i)  
		b1 = infor.b - e_i - \  
			 infor.y[i] * (infor.alpha[i] - alpha_i_old) * infor.K[i, i] - \  
			 infor.y[j] * (infor.alpha[j] - alpha_j_old) * infor.K[j, i]  
		b2 = infor.b - e_j - \  
			 infor.y[i] * (infor.alpha[i] - alpha_i_old) * infor.K[i, j] - \  
			 infor.y[j] * (infor.alpha[j] - alpha_j_old) * infor.K[j, j]
        if 0 < infor.alpha[i] < infor.c:  
            infor.b = b1  
        elif 0 < infor.alpha[j] < infor.c:  
            infor.b = b2  
        else:  
            infor.b = (b1 + b2) / 2.0  
        infor.alpha[i], infor.alpha[j] = alpha_i, alpha_j  
        return 1  
    else:  
        return 0
```
这个`optimize_alpha(infor, i)`可以根据传入的 i ，选择一个对应的合适的 j 进行配对以及优化。具体分为两步

一、选择合适的 j 
   这里调用的是`select_j(infor, i)`
   原则是<mark>尽量选择误差大的 j </mark>，因为误差大说明对应的 alpha可优化空间大。首次误差不可比较就随机选择 。
   ```python 
   def select_j(infor, i):  
	    """寻找最合适的 J """    
	    max_index, max_gap, max_e = -1, 0, 0
	    e_i = calculate_and_update_e(infor, i)  
	    cache_list = np.nonzero(infor.e_cache[:, 1])[0]  
	    if len(cache_list) > 1:  
	        for k in cache_list:  
	            if k == i: continue  
	            e_k = calculate_and_update_e(infor, k)  
	            gap = abs(e_i - e_k)  
	            if gap > max_gap:  
	                max_index = k  
	                max_gap = gap  
	                max_e = e_k  
	        return max_index, max_e  
	    else:  
	        j = np.random.choice([l for l in range(infor.m) if l != i])  
	        e_j = calculate_and_update_e(infor, j)  
	    return j, e_j
```
二、更新
   更新 $\alpha_i$ 使用的公式为：$$α_i=α_i+\frac{e_j-e_i}{\zeta},\quad \text{s.t.\quad} \zeta=K[i,i]+K[j,j]-2K[i,j]$$
   一对 alpha 同时更新保持平衡（$\theta$为定值）
   $$\alpha_iy^{(i)}+\alpha_jy^{(j)}=-\sum_{k!=i,j}^{n}\alpha_ky^{(k)}=\theta$$
   推得$a_j=-\alpha_i\frac{y_i}{y_j}+\frac{\theta}{y_j}$再进一步推得==$\alpha_j=(-y_i\alpha_i+\theta)y_j$==
   
   两个公式带入就成功更新 一对 alpha。
   代码分别对应`def optimize_alpha(infor, i)`里的：
   ```python 
   eta = infor.K[j, j] + infor.K[i, i] - 2 * infor.K[i, j]  
   alpha_i = infor.alpha[i] + infor.y[i] * (e_j - e_i) / (eta + 1e-5)
```
```python 
zeta = infor.alpha[i] * infor.y[i] + infor.alpha[j] * infor.y[j]
alpha_j = (zeta - infor.y[i] * alpha_i) / infor.y[j]
```
	
   我这里更新 一对 alpha 就更新了 b 。其实按理说当更新完所有 alpha 再来计算 b ，计算量会小些。但是对小数据来说计算开销没有想象的大，过程中更新 b 会令误差的计算更加准确，有助于 alpha 的更新，所以我选择忽视计算开销 更新 b 。
   当中调用的`calculate_and_update_e(infor, i) `
   ```python 
   def calculate_and_update_e(infor, k):  
    """计算误差 e ，更新误差表"""  
    e_k = np.sum(infor.y * infor.alpha * infor.K[:, k]) + infor.b - infor.y[k]  
    infor.e_cache[k] = [1, e_k]  
    return e_k
```
   #### 外循环
   我们思考一个问题，我们是否需要每次都一个不落地遍历所有的 alpha ？遍历一遍之后，我们的 alpha 分布在[0, C]。对于α=0样本，对应的是非支持向量；对于α=C的样本，对应的是误分类的支持向量。这两类的 alpha 优化效果不明显（0到下限无法减小，C到上限无法增大）。所以在遍历过所有的 alpha 之后，我们可以只遍历非边界的 alpha ，这样的数量就会减少。
   当非边界遍历发现 alpha 没有更新，可能就存在两种情况，要不就是更新好了，要不就是出问题了。所以我们在全遍历一遍，确保不出问题，如果全遍历仍旧没有 alpha 更新，我们就确定是更新好了，就可以结束外循环。
   代码如下：
   ```python 
   while iter < max_iter and ((alpha_changed > 0) or entry):  
    alpha_changed = 0  
    if entry:  
        for i in range(infor.m):  
            alpha_changed += optimize_alpha(infor, i)  
            # print(f'全样本遍历：第{iter}次迭代，样本{i}，alpha优化次数为{alpha_changed}……')  
        iter += 1  
    else:  
        alpha_index = np.nonzero((infor.alpha > 0) * (infor.alpha < c))[0]  
        for i in alpha_index:  
            alpha_changed += optimize_alpha(infor, i)  
            # print(f'非边界遍历：第{iter}次迭代，样本{i}，alpha优化次数为{alpha_changed}……')  
        iter += 1  
    if entry:  
        entry = False  
    elif alpha_changed == 0:  
        entry = True  
    # print(f'迭代次数：{iter}')
```
#### 整体 SMO
涉及到多次两两内积，我们可以提前算好并存起来，这样可以减少重复计算
```python 
def simple_poly_kernel(d):  
    def k(x, y):  
        return np.inner(x, y) ** d  
    return k  
  
def rbf_kernel(sigma):  
    def k(x, y):  
        return np.exp(-np.inner(x - y, x - y) / (2 * sigma ** 2))  
    return k  
  
def cos_kernel(x, y):  
    return np.inner(x, y) / np.linalg.norm(x, 2) / np.linalg.norm(y, 2)  
  
def sigmoid_kernel(beta, c):  
    def k(x, y):  
        return np.tanh(beta * np.linalg.norm(x, 2) / np.linalg.norm(y, 2))  
    return k
```
```python 
def pre_K(infor, ker):  
    """预先计算所有向量的两两内积，减少重复计算"""  
    for i in range(infor.m):  
        for j in range(infor.m):  
            infor.K[i, j] = ker(x[i], x[j])
```
SMO 的实现代码：
```python 
def SMO(x, y, tol, c, ker, max_iter):  
    infor = Information(x, y, tol, c)  
    pre_K(infor, ker)  
    iter, alpha_changed, entry = 0, 0, True  
    while iter < max_iter and ((alpha_changed > 0) or entry):  
        alpha_changed = 0  
        if entry:  
            for i in range(infor.m):  
                alpha_changed += optimize_alpha(infor, i)  
            iter += 1  
        else:  
            alpha_index = np.nonzero((infor.alpha > 0) * (infor.alpha < c))[0]  
            for i in alpha_index:  
                alpha_changed += optimize_alpha(infor, i)  
            iter += 1  
        if entry:  
            entry = False  
        elif alpha_changed == 0:  
            entry = True  
    return infor
```
### 效果呈现
评估采取两个数据，线性可分一个和现象不可分一个。

首先是线性可分数据（左图为自己实现，右图为调库实现）![[Pasted image 20250302161035.png]]
然后是线性不可分数据（前一张为自己实现，后一张为调库实现![[Pasted image 20250302193050.png]]![[Pasted image 20250302193135.png]]
### 总结反思

一、算法上：

1. 速度：
	我的做法是少次数全样本遍历，多次非边界遍历。也许我可以只遍历不符合KKT条件的样本，避免对已经符合条件的样本进行不必要的计算；
	我选择的是一次优化一对 alpha，其实可以增加每次优化的 alpha 个数
2. 准确率：
	可以交叉验证调整C值，以避免过拟合和欠拟合。

二、技巧上：
	写代码的时候应该边写边调试验证，不要写了很多了自以为完美。避免写了很多发现不符合预期再来修改，这个时候的修改是很痛苦的，因为有些错误可能是因为打快了手误，难以发现而且不必要。


## 回归树（回归）
### 概述
回归树是一种基于决策树的回归方法，主要用于预测连续的数值型输出。回归树通过递归地将数据集划分成若干子集，最终在叶节点上给出预测值。

通过数学推导，回归树的目标是通过递归划分来最小化每个子集的均方误差（MSE）。在每一轮划分时，我们选择一个特征和一个阈值，使得划分后的子集的均方误差最小。目标函数为：

$$
\min_{\theta} \sum_{j=1}^{m} \left( \frac{1}{n_j} \sum_{i=1}^{n_j} (y_i - \hat{y}_j)^2 \right)
$$
其中，$\hat{y}_j$是第 j 个子集的均值，$y_i$​ 是该子集的真实值，$n_j$​ 是该子集的样本数量。

在划分的过程中，我们通过计算每个特征与每个阈值的均方误差，选择最优的分裂点。回归树会继续对每个子集进行递归分裂，直到满足停止条件（如最大深度、最小样本数等）。

最终的预测函数为：

$$
f(x) = \sum_{j=1}^{k} \hat{y}_j I(x \in R_j)
$$

其中，$R_j$​ 表示第  j 个叶节点对应的区域，$\hat{y}_j$ 是该区域内样本的平均值.

为了防止过拟合，回归树通常会设置一些停止条件，如树的最大深度、节点的最小样本数或最小误差减少量。在实际应用中，回归树常与集成方法（如随机森林和梯度提升树）结合，以提高模型的准确性和稳定性。
### 逻辑推理

构建一颗树的过程其实就是不断分支的过程，关键点就在分支上面，不断迭代，就能拿到一棵树。

	满足分支条件：
		分支
	返回构建好的树

那我们需要考虑的就是分支条件是什么， 怎么构建树。
![[b3c3a69aefaa561bed5366e6df73369.jpg]]
### 代码实现

计算方差`variance(self, Y)`
```python
def variance(self, Y):  
    """计算某个子数据集的方差"""  
    if len(Y) == 0:  
        return 0  
    return np.var(Y)
```

计算方差减少量
`variance_reduction(self, X, Y, feat, val)`
```python
def variance_reduction(self, X, Y, feat, val):  
    """计算方差减少量"""  
    N = len(Y)  
    if N == 0:  
        return 0  
    var_before = self.variance(Y)  
    Y_l = Y[X[:, feat] <= val]  
    Y_r = Y[X[:, feat] > val]  
    var_after = (len(Y_l) / N) * self.variance(Y_l) + (len(Y_r) / N) * self.variance(Y_r)  
    return var_before - var_after
```
构建回归树`variance_reduction`
```python
def build_tree(self, node, X, Y, depth):  
    if len(Y) == 0:  
        return  
  
    # 如果所有 Y 值相同，则设为叶节点  
    if np.all(Y == Y[0]):  
        node.feat = np.mean(Y)  
        self.T += 1  
        return  
  
    # 如果达到最大深度，设置为叶节点  
    if self.max_depth is not None and depth >= self.max_depth:  
        node.feat = np.mean(Y)  
        self.T += 1  
        return  
  
    # 寻找最佳划分特征和划分点  
    best_reduction = 0  
    best_feat = None  
    best_val = None  
  
    for feat in range(X.shape[1]):  
        # 对于每个特征，选择唯一的划分值  
        unique_vals = np.unique(X[:, feat])  
        for val in unique_vals:  
            reduction = self.variance_reduction(X, Y, feat, val)  
            if reduction > best_reduction:  
                best_reduction = reduction  
                best_feat = feat  
                best_val = val  
  
    # 如果找到有效的划分  
    if best_feat is not None:  
        # 计算当前划分的代价  
        cur_cost = len(Y) * self.variance(Y) + self.lbd  
        Y_l = Y[X[:, best_feat] <= best_val]  
        Y_r = Y[X[:, best_feat] > best_val]  
        new_cost = len(Y_l) * self.variance(Y_l) + len(Y_r) * self.variance(Y_r) + 2 * self.lbd  
  
        if new_cost < cur_cost:  # 如果划分有效，则继续划分  
            node.feat = best_feat  
            node.split = best_val  
            l_child = Node()  
            r_child = Node()  
            self.build_tree(l_child, X[X[:, best_feat] <= best_val], Y[X[:, best_feat] <= best_val], depth + 1)  
            self.build_tree(r_child, X[X[:, best_feat] > best_val], Y[X[:, best_feat] > best_val], depth + 1)  
            node.child = [l_child, r_child]  
        else:  # 如果划分无效，则设为叶节点  
            node.feat = np.mean(Y)  
            self.T += 1  
    else:  # 无法继续划分时，将节点设为叶节点  
        node.feat = np.mean(Y)  
        self.T += 1
```
最后的预测函数``
```python
def predict(self, x):  
    node = self.root  
    while node.child:  
        if x[node.feat] <= node.split:  
            node = node.child[0]  
        else:  
            node = node.child[1]  
    return node.feat
```
### 效果呈现
上面是自己实现的效果，下面是调库：
![[Pasted image 20250303130539.png]]![[Pasted image 20250303130555.png]]
预测准确率比调库高，但运行时间差很远
### 总结反思

目前在 `variance_reduction` 函数中，使用了 `np.unique` 来计算每个特征的所有唯一值。这对于较大数据集来说可能会非常慢，尤其是特征数量多时。